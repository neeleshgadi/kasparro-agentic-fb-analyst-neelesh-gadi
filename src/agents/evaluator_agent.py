"""
Evaluator Agent for validating hypotheses using quantitative metrics.

This agent validates hypotheses generated by the Insight Agent through
statistical analysis, segmentation comparison, and confidence score adjustment.
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
from scipy import stats

from src.schemas.validation import validate_agent_input, validate_agent_output
from src.schemas.agent_io import EVALUATOR_INPUT_SCHEMA, EVALUATOR_OUTPUT_SCHEMA
from src.utils.logger import setup_logger


class EvaluatorAgent:
    """Agent responsible for validating hypotheses using dataset metrics."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize Evaluator Agent.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.agent_name = "evaluator_agent"
        self.logger = setup_logger(
            self.agent_name,
            log_level=config.get("logging", {}).get("level", "INFO"),
            log_format=config.get("logging", {}).get("format", "json"),
            log_dir=config.get("logging", {}).get("log_dir", "logs"),
        )
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute evaluator agent workflow.
        
        Args:
            input_data: Input containing hypotheses, dataset_path, data_summary, config
            
        Returns:
            Evaluator agent output with validated hypotheses and reasoning
        """
        start_time = datetime.utcnow()
        
        try:
            # Validate input
            validate_agent_input(input_data, EVALUATOR_INPUT_SCHEMA, self.agent_name)
            
            # Extract input parameters
            hypotheses = input_data["hypotheses"]
            dataset_path = input_data["dataset_path"]
            data_summary = input_data["data_summary"]
            
            # Load dataset
            df = self._load_dataset(dataset_path)
            
            # Validate each hypothesis
            validated_hypotheses = []
            for hypothesis in hypotheses:
                validated = self._validate_hypothesis(hypothesis, df, data_summary)
                validated_hypotheses.append(validated)
            
            # Rank hypotheses by validation strength and confidence
            validated_hypotheses = self._rank_hypotheses(validated_hypotheses)
            
            # Extract top insights
            top_insights = self._extract_top_insights(validated_hypotheses)
            
            # Generate reasoning
            reasoning = self._generate_reasoning(hypotheses, validated_hypotheses)
            
            # Calculate execution duration
            execution_duration_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
            
            # Build output
            output = {
                "agent_name": self.agent_name,
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "execution_duration_ms": execution_duration_ms,
                "validated_hypotheses": validated_hypotheses,
                "top_insights": top_insights,
                "reasoning": reasoning,
            }
            
            # Validate output
            validate_agent_output(output, EVALUATOR_OUTPUT_SCHEMA, self.agent_name)
            
            self.logger.info(
                f"Evaluator Agent completed successfully",
                extra={
                    "agent_name": self.agent_name,
                    "execution_duration_ms": execution_duration_ms,
                    "hypotheses_validated": len(validated_hypotheses),
                }
            )
            
            return output
            
        except Exception as e:
            self.logger.error(
                f"Evaluator Agent execution failed: {str(e)}",
                extra={"agent_name": self.agent_name, "error_type": type(e).__name__},
                exc_info=True
            )
            raise

    
    def _load_dataset(self, dataset_path: str) -> pd.DataFrame:
        """Load dataset from CSV.
        
        Args:
            dataset_path: Path to CSV file
            
        Returns:
            DataFrame with loaded data
        """
        if not Path(dataset_path).exists():
            raise FileNotFoundError(f"Dataset file not found: {dataset_path}")
        
        df = pd.read_csv(dataset_path)
        
        # Parse dates
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors='coerce')
        
        # Ensure numeric fields
        numeric_fields = ["spend", "impressions", "clicks", "revenue", "purchases", "ctr", "roas"]
        for field in numeric_fields:
            if field in df.columns:
                df[field] = pd.to_numeric(df[field], errors='coerce')
        
        return df
    
    def _validate_hypothesis(
        self,
        hypothesis: Dict[str, Any],
        df: pd.DataFrame,
        data_summary: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validate a single hypothesis using dataset metrics.
        
        Args:
            hypothesis: Hypothesis dictionary from Insight Agent
            df: Dataset DataFrame
            data_summary: Data summary from Data Agent
            
        Returns:
            Validated hypothesis with evidence and adjusted confidence
        """
        hypothesis_id = hypothesis.get("hypothesis_id", "unknown")
        hypothesis_text = hypothesis.get("hypothesis_text", "")
        category = hypothesis.get("category", "unknown")
        initial_confidence = hypothesis.get("confidence_score", 0.5)
        
        # Determine validation approach based on category
        if category == "creative":
            evidence, validation_status = self._validate_creative_hypothesis(hypothesis, df, data_summary)
        elif category == "audience":
            evidence, validation_status = self._validate_audience_hypothesis(hypothesis, df, data_summary)
        elif category == "platform":
            evidence, validation_status = self._validate_platform_hypothesis(hypothesis, df, data_summary)
        elif category == "budget":
            evidence, validation_status = self._validate_budget_hypothesis(hypothesis, df, data_summary)
        elif category == "seasonality":
            evidence, validation_status = self._validate_seasonality_hypothesis(hypothesis, df, data_summary)
        else:
            evidence, validation_status = self._validate_generic_hypothesis(hypothesis, df, data_summary)
        
        # Calculate validation strength
        validation_strength = self._calculate_validation_strength(evidence, validation_status)
        
        # Calculate segmentation evidence score
        segmentation_evidence = self._calculate_segmentation_evidence(evidence)
        
        # Adjust confidence score using weighted formula
        adjusted_confidence = self._adjust_confidence_score(
            initial_confidence,
            validation_strength,
            segmentation_evidence
        )
        
        # Generate validation reasoning
        validation_reasoning = self._generate_validation_reasoning(
            hypothesis_text,
            evidence,
            validation_status,
            adjusted_confidence
        )
        
        return {
            "hypothesis_id": hypothesis_id,
            "hypothesis_text": hypothesis_text,
            "validation_status": validation_status,
            "evidence": evidence,
            "adjusted_confidence_score": adjusted_confidence,
            "validation_reasoning": validation_reasoning
        }

    
    def _validate_creative_hypothesis(
        self,
        hypothesis: Dict[str, Any],
        df: pd.DataFrame,
        data_summary: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], str]:
        """Validate creative-related hypothesis.
        
        Args:
            hypothesis: Hypothesis dictionary
            df: Dataset DataFrame
            data_summary: Data summary
            
        Returns:
            Tuple of (evidence dict, validation status)
        """
        metrics = []
        
        # Get segmentation data
        segmentation = data_summary.get("segmentation", {})
        creative_segments = segmentation.get("by_creative_type", [])
        
        if len(creative_segments) >= 2:
            # Compare creative types
            creative_segments_sorted = sorted(creative_segments, key=lambda x: x.get("ctr", 0))
            worst_creative = creative_segments_sorted[0]
            best_creative = creative_segments_sorted[-1]
            
            metrics.append({
                "metric_name": "worst_creative_ctr",
                "value": worst_creative.get("ctr", 0),
                "comparison": f"Worst performing creative type: {worst_creative.get('creative_type', 'unknown')}"
            })
            
            metrics.append({
                "metric_name": "best_creative_ctr",
                "value": best_creative.get("ctr", 0),
                "comparison": f"Best performing creative type: {best_creative.get('creative_type', 'unknown')}"
            })
            
            # Calculate statistical significance if possible
            statistical_significance = self._calculate_statistical_significance(
                df,
                "creative_type",
                worst_creative.get("creative_type"),
                best_creative.get("creative_type"),
                "ctr"
            )
            
            # Determine validation status
            ctr_diff = best_creative.get("ctr", 0) - worst_creative.get("ctr", 0)
            if ctr_diff > 0.002:  # Significant CTR difference
                validation_status = "confirmed"
            elif ctr_diff < 0:
                validation_status = "rejected"
            else:
                validation_status = "inconclusive"
        else:
            # Insufficient data
            metrics.append({
                "metric_name": "creative_segments_count",
                "value": len(creative_segments),
                "comparison": "Insufficient creative type segments for comparison"
            })
            metrics.append({
                "metric_name": "overall_ctr",
                "value": data_summary.get("metrics", {}).get("overall_ctr", 0),
                "comparison": "Overall CTR baseline"
            })
            validation_status = "inconclusive"
            statistical_significance = None
        
        evidence = {
            "metrics": metrics,
        }
        
        if statistical_significance:
            evidence["statistical_significance"] = statistical_significance
        
        return evidence, validation_status

    
    def _validate_audience_hypothesis(
        self,
        hypothesis: Dict[str, Any],
        df: pd.DataFrame,
        data_summary: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], str]:
        """Validate audience-related hypothesis.
        
        Args:
            hypothesis: Hypothesis dictionary
            df: Dataset DataFrame
            data_summary: Data summary
            
        Returns:
            Tuple of (evidence dict, validation status)
        """
        metrics = []
        
        # Get segmentation data
        segmentation = data_summary.get("segmentation", {})
        audience_segments = segmentation.get("by_audience_type", [])
        
        if len(audience_segments) >= 2:
            # Compare audience types
            audience_segments_sorted = sorted(audience_segments, key=lambda x: x.get("roas", 0))
            worst_audience = audience_segments_sorted[0]
            best_audience = audience_segments_sorted[-1]
            
            metrics.append({
                "metric_name": "worst_audience_roas",
                "value": worst_audience.get("roas", 0),
                "comparison": f"Worst performing audience: {worst_audience.get('audience_type', 'unknown')}"
            })
            
            metrics.append({
                "metric_name": "best_audience_roas",
                "value": best_audience.get("roas", 0),
                "comparison": f"Best performing audience: {best_audience.get('audience_type', 'unknown')}"
            })
            
            # Calculate statistical significance
            statistical_significance = self._calculate_statistical_significance(
                df,
                "audience_type",
                worst_audience.get("audience_type"),
                best_audience.get("audience_type"),
                "roas"
            )
            
            # Determine validation status
            roas_diff = best_audience.get("roas", 0) - worst_audience.get("roas", 0)
            if roas_diff > 0.5:  # Significant ROAS difference
                validation_status = "confirmed"
            elif roas_diff < 0:
                validation_status = "rejected"
            else:
                validation_status = "inconclusive"
        else:
            # Insufficient data
            metrics.append({
                "metric_name": "audience_segments_count",
                "value": len(audience_segments),
                "comparison": "Insufficient audience segments for comparison"
            })
            metrics.append({
                "metric_name": "overall_roas",
                "value": data_summary.get("metrics", {}).get("overall_roas", 0),
                "comparison": "Overall ROAS baseline"
            })
            validation_status = "inconclusive"
            statistical_significance = None
        
        evidence = {
            "metrics": metrics,
        }
        
        if statistical_significance:
            evidence["statistical_significance"] = statistical_significance
        
        return evidence, validation_status

    
    def _validate_platform_hypothesis(
        self,
        hypothesis: Dict[str, Any],
        df: pd.DataFrame,
        data_summary: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], str]:
        """Validate platform-related hypothesis.
        
        Args:
            hypothesis: Hypothesis dictionary
            df: Dataset DataFrame
            data_summary: Data summary
            
        Returns:
            Tuple of (evidence dict, validation status)
        """
        metrics = []
        
        # Get segmentation data
        segmentation = data_summary.get("segmentation", {})
        platform_segments = segmentation.get("by_platform", [])
        
        if len(platform_segments) >= 2:
            # Compare platforms
            platform_segments_sorted = sorted(platform_segments, key=lambda x: x.get("roas", 0))
            worst_platform = platform_segments_sorted[0]
            best_platform = platform_segments_sorted[-1]
            
            metrics.append({
                "metric_name": "worst_platform_roas",
                "value": worst_platform.get("roas", 0),
                "comparison": f"Worst performing platform: {worst_platform.get('platform', 'unknown')}"
            })
            
            metrics.append({
                "metric_name": "best_platform_roas",
                "value": best_platform.get("roas", 0),
                "comparison": f"Best performing platform: {best_platform.get('platform', 'unknown')}"
            })
            
            # Calculate statistical significance
            statistical_significance = self._calculate_statistical_significance(
                df,
                "platform",
                worst_platform.get("platform"),
                best_platform.get("platform"),
                "roas"
            )
            
            # Determine validation status
            roas_diff = best_platform.get("roas", 0) - worst_platform.get("roas", 0)
            if roas_diff > 0.3:  # Significant ROAS difference
                validation_status = "confirmed"
            elif roas_diff < 0:
                validation_status = "rejected"
            else:
                validation_status = "inconclusive"
        else:
            # Insufficient data
            metrics.append({
                "metric_name": "platform_segments_count",
                "value": len(platform_segments),
                "comparison": "Insufficient platform segments for comparison"
            })
            metrics.append({
                "metric_name": "overall_roas",
                "value": data_summary.get("metrics", {}).get("overall_roas", 0),
                "comparison": "Overall ROAS baseline"
            })
            validation_status = "inconclusive"
            statistical_significance = None
        
        evidence = {
            "metrics": metrics,
        }
        
        if statistical_significance:
            evidence["statistical_significance"] = statistical_significance
        
        return evidence, validation_status

    
    def _validate_budget_hypothesis(
        self,
        hypothesis: Dict[str, Any],
        df: pd.DataFrame,
        data_summary: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], str]:
        """Validate budget-related hypothesis.
        
        Args:
            hypothesis: Hypothesis dictionary
            df: Dataset DataFrame
            data_summary: Data summary
            
        Returns:
            Tuple of (evidence dict, validation status)
        """
        metrics = []
        
        # Get segmentation data
        segmentation = data_summary.get("segmentation", {})
        campaign_segments = segmentation.get("by_campaign", [])
        
        if len(campaign_segments) >= 2:
            # Compare campaigns
            campaign_segments_sorted = sorted(campaign_segments, key=lambda x: x.get("roas", 0))
            worst_campaign = campaign_segments_sorted[0]
            best_campaign = campaign_segments_sorted[-1]
            
            metrics.append({
                "metric_name": "worst_campaign_roas",
                "value": worst_campaign.get("roas", 0),
                "comparison": f"Worst performing campaign: {worst_campaign.get('campaign_name', 'unknown')}"
            })
            
            metrics.append({
                "metric_name": "best_campaign_roas",
                "value": best_campaign.get("roas", 0),
                "comparison": f"Best performing campaign: {best_campaign.get('campaign_name', 'unknown')}"
            })
            
            # Add spend information
            metrics.append({
                "metric_name": "worst_campaign_spend",
                "value": worst_campaign.get("spend", 0),
                "comparison": f"Spend on worst campaign: ${worst_campaign.get('spend', 0):.2f}"
            })
            
            # Determine validation status
            roas_diff = best_campaign.get("roas", 0) - worst_campaign.get("roas", 0)
            if roas_diff > 0.5:  # Significant ROAS difference
                validation_status = "confirmed"
            elif roas_diff < 0:
                validation_status = "rejected"
            else:
                validation_status = "inconclusive"
        else:
            # Insufficient data
            metrics.append({
                "metric_name": "campaign_segments_count",
                "value": len(campaign_segments),
                "comparison": "Insufficient campaign segments for comparison"
            })
            metrics.append({
                "metric_name": "overall_roas",
                "value": data_summary.get("metrics", {}).get("overall_roas", 0),
                "comparison": "Overall ROAS baseline"
            })
            validation_status = "inconclusive"
        
        evidence = {
            "metrics": metrics,
        }
        
        return evidence, validation_status

    
    def _validate_seasonality_hypothesis(
        self,
        hypothesis: Dict[str, Any],
        df: pd.DataFrame,
        data_summary: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], str]:
        """Validate seasonality-related hypothesis.
        
        Args:
            hypothesis: Hypothesis dictionary
            df: Dataset DataFrame
            data_summary: Data summary
            
        Returns:
            Tuple of (evidence dict, validation status)
        """
        metrics = []
        
        # Get trend data
        trends = data_summary.get("trends", {})
        roas_trend = trends.get("roas_trend", {})
        ctr_trend = trends.get("ctr_trend", {})
        
        # Add trend metrics
        metrics.append({
            "metric_name": "roas_wow_change",
            "value": roas_trend.get("week_over_week_change", 0),
            "comparison": f"ROAS week-over-week change: {roas_trend.get('week_over_week_change', 0):.1f}%"
        })
        
        metrics.append({
            "metric_name": "roas_mom_change",
            "value": roas_trend.get("month_over_month_change", 0),
            "comparison": f"ROAS month-over-month change: {roas_trend.get('month_over_month_change', 0):.1f}%"
        })
        
        metrics.append({
            "metric_name": "ctr_wow_change",
            "value": ctr_trend.get("week_over_week_change", 0),
            "comparison": f"CTR week-over-week change: {ctr_trend.get('week_over_week_change', 0):.1f}%"
        })
        
        # Determine validation status based on trend magnitude
        avg_change = (
            abs(roas_trend.get("week_over_week_change", 0)) +
            abs(roas_trend.get("month_over_month_change", 0))
        ) / 2
        
        if avg_change > 10:  # Significant trend
            validation_status = "confirmed"
        elif avg_change < 3:  # Minimal trend
            validation_status = "rejected"
        else:
            validation_status = "inconclusive"
        
        evidence = {
            "metrics": metrics,
        }
        
        return evidence, validation_status
    
    def _validate_generic_hypothesis(
        self,
        hypothesis: Dict[str, Any],
        df: pd.DataFrame,
        data_summary: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], str]:
        """Validate generic hypothesis with basic metrics.
        
        Args:
            hypothesis: Hypothesis dictionary
            df: Dataset DataFrame
            data_summary: Data summary
            
        Returns:
            Tuple of (evidence dict, validation status)
        """
        metrics = []
        
        # Add overall metrics
        overall_metrics = data_summary.get("metrics", {})
        metrics.append({
            "metric_name": "overall_roas",
            "value": overall_metrics.get("overall_roas", 0),
            "comparison": "Overall ROAS"
        })
        
        metrics.append({
            "metric_name": "overall_ctr",
            "value": overall_metrics.get("overall_ctr", 0),
            "comparison": "Overall CTR"
        })
        
        evidence = {
            "metrics": metrics,
        }
        
        validation_status = "inconclusive"
        
        return evidence, validation_status

    
    def _calculate_statistical_significance(
        self,
        df: pd.DataFrame,
        segment_field: str,
        segment1_value: str,
        segment2_value: str,
        metric: str
    ) -> Optional[Dict[str, Any]]:
        """Calculate statistical significance between two segments.
        
        Args:
            df: Dataset DataFrame
            segment_field: Field to segment by
            segment1_value: Value for first segment
            segment2_value: Value for second segment
            metric: Metric to compare
            
        Returns:
            Dictionary with p_value and confidence_interval, or None if insufficient data
        """
        if segment_field not in df.columns or metric not in df.columns:
            return None
        
        # Get data for each segment
        segment1_data = df[df[segment_field] == segment1_value][metric].dropna()
        segment2_data = df[df[segment_field] == segment2_value][metric].dropna()
        
        # Need at least 10 data points in each segment for meaningful statistics
        min_sample_size = self.config.get("agents", {}).get("min_data_points", 10)
        if len(segment1_data) < min_sample_size or len(segment2_data) < min_sample_size:
            return None
        
        try:
            # Perform t-test
            t_stat, p_value = stats.ttest_ind(segment1_data, segment2_data)
            
            # Calculate confidence interval for the difference
            mean_diff = segment2_data.mean() - segment1_data.mean()
            std_error = np.sqrt(
                (segment1_data.var() / len(segment1_data)) +
                (segment2_data.var() / len(segment2_data))
            )
            
            # 95% confidence interval
            ci_lower = mean_diff - 1.96 * std_error
            ci_upper = mean_diff + 1.96 * std_error
            
            return {
                "p_value": float(p_value),
                "confidence_interval": [float(ci_lower), float(ci_upper)]
            }
        except Exception as e:
            self.logger.warning(
                f"Failed to calculate statistical significance: {str(e)}",
                extra={"segment_field": segment_field, "metric": metric}
            )
            return None
    
    def _calculate_validation_strength(
        self,
        evidence: Dict[str, Any],
        validation_status: str
    ) -> float:
        """Calculate validation strength score.
        
        Args:
            evidence: Evidence dictionary
            validation_status: Validation status (confirmed/rejected/inconclusive)
            
        Returns:
            Validation strength score between 0 and 1
        """
        # Base score from validation status
        if validation_status == "confirmed":
            base_score = 0.7
        elif validation_status == "rejected":
            base_score = 0.2
        else:  # inconclusive
            base_score = 0.4
        
        # Adjust based on evidence quality
        metrics = evidence.get("metrics", [])
        has_statistical_significance = "statistical_significance" in evidence
        
        # Bonus for having statistical significance
        if has_statistical_significance:
            stat_sig = evidence["statistical_significance"]
            p_value = stat_sig.get("p_value", 1.0)
            if p_value < 0.05:
                base_score = min(1.0, base_score + 0.2)
            elif p_value < 0.1:
                base_score = min(1.0, base_score + 0.1)
        
        # Penalty for insufficient metrics
        if len(metrics) < 2:
            base_score = max(0.0, base_score - 0.2)
        
        return max(0.0, min(1.0, base_score))
    
    def _calculate_segmentation_evidence(self, evidence: Dict[str, Any]) -> float:
        """Calculate segmentation evidence score.
        
        Args:
            evidence: Evidence dictionary
            
        Returns:
            Segmentation evidence score between 0 and 1
        """
        metrics = evidence.get("metrics", [])
        
        # Count comparison metrics (metrics that compare segments)
        comparison_metrics = [m for m in metrics if "comparison" in m and "vs" in m.get("comparison", "").lower()]
        
        if len(metrics) >= 2 and len(comparison_metrics) >= 1:
            return 0.8
        elif len(metrics) >= 2:
            return 0.6
        elif len(metrics) >= 1:
            return 0.4
        else:
            return 0.2
    
    def _adjust_confidence_score(
        self,
        initial_confidence: float,
        validation_strength: float,
        segmentation_evidence: float
    ) -> float:
        """Adjust confidence score using weighted formula.
        
        Formula: (InsightConfidence * 0.4) + (ValidationStrength * 0.4) + (SegmentationEvidence * 0.2)
        
        Args:
            initial_confidence: Initial confidence from Insight Agent
            validation_strength: Validation strength score
            segmentation_evidence: Segmentation evidence score
            
        Returns:
            Adjusted confidence score between 0 and 1
        """
        adjusted = (
            (initial_confidence * 0.4) +
            (validation_strength * 0.4) +
            (segmentation_evidence * 0.2)
        )
        
        return max(0.0, min(1.0, adjusted))

    
    def _generate_validation_reasoning(
        self,
        hypothesis_text: str,
        evidence: Dict[str, Any],
        validation_status: str,
        adjusted_confidence: float
    ) -> str:
        """Generate validation reasoning text.
        
        Args:
            hypothesis_text: Hypothesis text
            evidence: Evidence dictionary
            validation_status: Validation status
            adjusted_confidence: Adjusted confidence score
            
        Returns:
            Validation reasoning string
        """
        reasoning = f"Hypothesis validation: {validation_status}. "
        
        metrics = evidence.get("metrics", [])
        if metrics:
            reasoning += f"Analyzed {len(metrics)} metrics. "
            
            # Mention key metrics
            for metric in metrics[:2]:
                reasoning += f"{metric.get('comparison', '')}. "
        
        # Mention statistical significance if available
        if "statistical_significance" in evidence:
            stat_sig = evidence["statistical_significance"]
            p_value = stat_sig.get("p_value", 1.0)
            reasoning += f"Statistical test p-value: {p_value:.4f}. "
            
            if p_value < 0.05:
                reasoning += "Result is statistically significant. "
            else:
                reasoning += "Result is not statistically significant. "
        
        # Explain confidence adjustment
        reasoning += f"Adjusted confidence score: {adjusted_confidence:.2f}. "
        
        if adjusted_confidence >= 0.7:
            reasoning += "High confidence in this hypothesis."
        elif adjusted_confidence >= 0.5:
            reasoning += "Moderate confidence in this hypothesis."
        else:
            reasoning += "Low confidence - requires further investigation."
        
        return reasoning
    
    def _rank_hypotheses(self, validated_hypotheses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Rank hypotheses by validation strength and confidence.
        
        Args:
            validated_hypotheses: List of validated hypothesis dictionaries
            
        Returns:
            Sorted list of validated hypotheses (highest confidence first)
        """
        # Sort by adjusted confidence score (descending)
        return sorted(
            validated_hypotheses,
            key=lambda h: h["adjusted_confidence_score"],
            reverse=True
        )
    
    def _extract_top_insights(self, validated_hypotheses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract top insights from validated hypotheses.
        
        Args:
            validated_hypotheses: List of validated hypothesis dictionaries
            
        Returns:
            List of top insights (top 3)
        """
        top_insights = []
        
        # Take top 3 hypotheses
        for hypothesis in validated_hypotheses[:3]:
            top_insights.append({
                "hypothesis": hypothesis["hypothesis_text"],
                "validated_confidence": hypothesis["adjusted_confidence_score"]
            })
        
        return top_insights
    
    def _generate_reasoning(
        self,
        hypotheses: List[Dict[str, Any]],
        validated_hypotheses: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """Generate reasoning structure for evaluator agent output.
        
        Args:
            hypotheses: Original hypotheses from Insight Agent
            validated_hypotheses: Validated hypotheses
            
        Returns:
            Reasoning dict with think, analyze, conclude sections
        """
        # Think section
        think = f"Validating {len(hypotheses)} hypotheses using quantitative metrics and statistical analysis. "
        think += "Each hypothesis will be tested against dataset segments and compared for statistical significance. "
        think += "Confidence scores will be adjusted based on validation strength and segmentation evidence."
        
        # Analyze section
        analyze = "Validation analysis:\n"
        
        # Count validation statuses
        confirmed = sum(1 for h in validated_hypotheses if h["validation_status"] == "confirmed")
        rejected = sum(1 for h in validated_hypotheses if h["validation_status"] == "rejected")
        inconclusive = sum(1 for h in validated_hypotheses if h["validation_status"] == "inconclusive")
        
        analyze += f"- Confirmed hypotheses: {confirmed}\n"
        analyze += f"- Rejected hypotheses: {rejected}\n"
        analyze += f"- Inconclusive hypotheses: {inconclusive}\n"
        
        # Count statistical significance tests
        with_stats = sum(1 for h in validated_hypotheses if "statistical_significance" in h.get("evidence", {}))
        analyze += f"- Hypotheses with statistical significance tests: {with_stats}\n"
        
        # Confidence score distribution
        high_conf = sum(1 for h in validated_hypotheses if h["adjusted_confidence_score"] >= 0.7)
        med_conf = sum(1 for h in validated_hypotheses if 0.5 <= h["adjusted_confidence_score"] < 0.7)
        low_conf = sum(1 for h in validated_hypotheses if h["adjusted_confidence_score"] < 0.5)
        
        analyze += f"- High confidence (â‰¥0.7): {high_conf}\n"
        analyze += f"- Medium confidence (0.5-0.7): {med_conf}\n"
        analyze += f"- Low confidence (<0.5): {low_conf}"
        
        # Conclude section
        conclude = f"Validation complete for {len(validated_hypotheses)} hypotheses. "
        
        if validated_hypotheses:
            top_hypothesis = validated_hypotheses[0]
            conclude += f"Top hypothesis (confidence: {top_hypothesis['adjusted_confidence_score']:.2f}): "
            conclude += f"{top_hypothesis['hypothesis_text']}. "
            conclude += f"Status: {top_hypothesis['validation_status']}. "
        
        conclude += f"Generated {len(validated_hypotheses[:3])} top insights for reporting. "
        conclude += "Hypotheses ranked by adjusted confidence score using weighted formula: "
        conclude += "(InsightConfidence * 0.4) + (ValidationStrength * 0.4) + (SegmentationEvidence * 0.2)."
        
        return {
            "think": think,
            "analyze": analyze,
            "conclude": conclude
        }
